{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes is classification algorithm that exploits the product rule in probability theory to determine the class of an object by comparing the probability that it comes from different distributions. The probability of which an unknown data sample $k$ belongs to class $C$ is given by:\n",
    "\n",
    "$P(C_{k}|\\textbf{x}) = \\frac{P(C_{k})P(\\textbf{x}|C_{k})}{P(\\textbf{x})}$\n",
    "\n",
    "In practice, it is extremely difficult to calculate the probability in the denominator. Luckily, it is the same for all classes and thus can be regarded as a constant which can be omitted. So we only need to pay attention to the numerator. Assume that there are $N$ samples, $k$ of which belong to class $C$, then it is easy to derive the prior probability:\n",
    "\n",
    "$P(C_{k}) = \\frac{k}{N}$\n",
    "\n",
    "In order to compute the posterior probability, we assume the samples are retrieved from Gaussian distribution whose mean and covariance are the mean and covariance of the whole training dataset. Thus, the posterior probability can be computed as:\n",
    "\n",
    "$P(\\textbf{x}|C_{k}) = \\textit{N}(\\mu, \\sigma^{2})$\n",
    "\n",
    "In order to avoid numerical underflow, it is typical to take the logarithmic form of the probability(note that logarithmic function is monotonic), so the final formula becomes:\n",
    "\n",
    "$log(P(C_{k}|\\textbf{x})) = log(P(C_{k})) + log(P(x_{1}|C_{k})) + log(P(x_{2}|C_{k})) + ... + log(P(x_{t}|C_{k}))$\n",
    "\n",
    "where $t$ is the dimension of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('pima-indians-diabetes.csv', header = None).values\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "split = int(0.8*data.shape[0]) # randomly split the data set into 80% training and 20% testing\n",
    "acc = [] \n",
    "\n",
    "# perform cross validation 10 times\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    rand_idx = np.arange(data.shape[0])\n",
    "    np.random.shuffle(rand_idx) \n",
    "    train_data = data[rand_idx[0:split]]\n",
    "    test_data = data[rand_idx[split:]]\n",
    "    \n",
    "    # data is either labeled as \"1\"(positive) or \"0\"(negative)\n",
    "    train_pos = train_data[np.where(train_data.T[-1] == 1)]\n",
    "    train_neg = train_data[np.where(train_data.T[-1] == 0)]\n",
    "    test_pos = test_data[np.where(test_data.T[-1] == 1)]\n",
    "    test_neg = test_data[np.where(test_data.T[-1] == 0)]\n",
    "    \n",
    "    # calculate mean and covariance of positive and negative data samples, respectively\n",
    "    pos_mean = np.nanmean(train_pos, axis=0)\n",
    "    pos_var = np.nanvar(train_pos, axis=0)\n",
    "    neg_mean = np.nanmean(train_neg, axis=0)\n",
    "    neg_var = np.nanvar(train_neg, axis=0)\n",
    "    \n",
    "    err = 0\n",
    "    \n",
    "    # calculate prior logarithmic probability\n",
    "    pos_prior_prob = log(test_pos.shape[0]/test_data.shape[0])\n",
    "    neg_prior_prob = log(test_neg.shape[0]/test_data.shape[0])\n",
    "    \n",
    "    for row in test_data:         \n",
    "        \n",
    "        # compute and compare the probability of both cases to predict label of test data\n",
    "        pos_prob = pos_prior_prob + log(multivariate_normal.pdf(row[0:-1], pos_mean[0:-1], pos_var[0:-1]))\n",
    "        neg_prob = neg_prior_prob + log(multivariate_normal.pdf(row[0:-1], neg_mean[0:-1], neg_var[0:-1]))\n",
    "\n",
    "        if pos_prob >= neg_prob:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "    \n",
    "        if label != row[-1]:\n",
    "            err += 1\n",
    "            \n",
    "    acc.append(1 - err/test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7727272727272727, 0.7987012987012987, 0.7142857142857143, 0.7402597402597403, 0.7207792207792207, 0.7727272727272727, 0.7402597402597403, 0.7727272727272727, 0.6818181818181819, 0.7922077922077921]\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
